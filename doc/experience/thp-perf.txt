= Linux Transparent Huge Pages (THPs) Performance Issues =

Transparent Huge Pages in Linux is aimed to resolve performance
issues on systems with large amounts of memory, like 64GB, since
the overhead of bookkeeping normal 4KB pages on such systems becomes
high. But it appears that sometimes the result is exactly the
opposite.

The problem with huge pages is that as a system continuously runs
its memory becomes very fragmented and it is hard to find a continuous
and aligned free chunk of memory (at least 2MB) for the huge page.
So the kernel tries to defragment the memory and this is when the
performance of the system starts to suffer  dramatically.

The problem was observed on an HPC cluster running CentOS 7.0.1406
with kernel version 3.10.0-229.14.1.el7.x86_64. Each node had 20
CPU Cores and 64GB of memory. Motr was running under the load tests
for days.  After some time a strange drops in performance started
appearing. Firstly time after time, but the longer the cluster was
running the more often were the drops and for longer the periods
they lasted. Eventually, the tests average results showed about 80%
degradation of original performance level.

It was noticed that during the drops some application threads were
stuck in consuming up to 100% of CPU system time (i.e. running the
kernel code). With perf utility we got the following stack trace:

Samples: 124K of event 'cycles', Event count (approx.): 53873309600
-   2.76%     m0d  [kernel.kallsyms]   [k] generic_exec_single
   - generic_exec_single
  	- 99.92% smp_call_function_single
           smp_call_function_many
           native_flush_tlb_others
           flush_tlb_page
           ptep_clear_flush
           try_to_unmap_one
           try_to_unmap_anon
           try_to_unmap
           migrate_pages
           compact_zone
           compact_zone_order
           try_to_compact_pages
           __alloc_pages_direct_compact
           __alloc_pages_nodemask
           alloc_pages_vma
           do_huge_pmd_anonymous_page
           handle_mm_fault
           __do_page_fault
           do_page_fault
         - page_fault
              99.66% new_heap

The situation looked exactly as it is described in the
Linux kernel's transhuge.txt document [1]:

	As the system ages, allocating huge pages may be expensive
	as the system uses memory compaction to copy data around
	memory to free a huge page for use. There are some counters
	in /proc/vmstat to help monitor this overhead.
	...
	compact_fail is incremented if the system tries to compact
	memory but failed.

And indeed the compact_fail counter was noticed incrementing.

Also in the same document it is written:

	It's also possible to limit defrag efforts in the VM to
	generate hugepages in case they're not immediately free to
	madvise regions or to never try to defrag memory and simply
	fallback to regular pages unless hugepages are immediately
	available.

So we disabled defrag on all the cluster nodes (without restarting)
and the problem just disappeared:

echo never > /sys/kernel/mm/transparent_hugepage/defrag

(By default it was set to always.)

References:
[1] Main document about THPs in Linux:
    https://www.kernel.org/doc/Documentation/vm/transhuge.txt
[2] Similar problem with bunch of useful links:
    https://www.ghostar.org/2015/02/transparent-huge-pages-on-hadoop-makes-me-sad
[3] Looks like in CentOS 6.4 defrag was disabled by default to cope
    with exactly the same issue: https://bugs.centos.org/view.php?id=5716
 
